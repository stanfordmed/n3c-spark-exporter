# gcp configurations
project_id:  gcp-project-id
service_account:  Service account email
region:  gcp-region
subnetwork_uri: projects/gcp-project-id/regions/gcp-region/subnetworks/gcp-subnetwork
gcs_bucket: gcp-bucket
# recommended to use a prefix for gcs files generated by this script
prefix: extractor
output_dir: local-folder-to-copy-csvs-to

# deid, cohorts and subset datasets created by n3c scripts, please run 
cdm_database_schema: gcp-project-id.deid-dataset-name
results_database_schema: gcp-project-id.cohorts-dataset-name
# make sure all cdm tables are created and populated
persist_database_schema: gcp-project-id.subset-dataset-name

# pyspark configurations
timeout_in_min: 120

# channge the values for your environment
site_abbrev: STARR_OMOP
site_name: site-name
contact_name: firstname lastname
contact_email: email
# cdm name (pcornet, omop, act)
cdm_name: OMOP
cdm_version: 5.3.1
vocabulary_version: v5.0 2022-06-09
# set below to Y or N
n3c_phenotype_yn: Y
# latency days between (current) run date and clarity date
data_latency_num_days: 14 
days_between_submissions: 0
# set below to Y or N
shift_date_yn: N
# set below to number of days, UNKNOWN if shifting and not known, NA if not shifting
max_num_shift_days: NA

spark_bq_jar: gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.13-0.31.0.jar
# if set to 0, it will leave the merged csv files for CDM tables in GCS, default is 1 i.e. csv files will be deleted
delete_merged_csvs_from_bucket: 0

# if set to 0, do not use CRC validation on export. Any non-zero number, use crc validation on export. Default = 0 (no CRC validation)
# Requires google-crc32c module installed
use_crc: 1


additional_cdm_tables: note,note_nlp,visit_detail

note: |
  select ...;

note_nlp: |
  select ...;

visit_detail: |
  select ...;